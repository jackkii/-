## 线性模型

### 一般公式：
y = w[0]x[0] + w[1]x[1] + ... + w[p]x[p] + b      
其中, x[0],x[1]...x[p]为数据集中特征变量的数量；w, b 为模型的参数； y为数据结果预测值
#### 线性模型通过训练数据集确定自身系数和截距
当训练集特征变量大于数据点数量时，线性模型可以对训练数据较好的预测
##
### 线性回归（普通最小二乘法）
当找到训练数据集中y的预测值和其真实值平方差最小的时候返回对应的w和b, 无可调节参数
### 岭回归（改良最小二乘法）
（训练结果可能模型在训练集的得分变小，而测试集的得分比训练集略高）  
  能够避免过拟合， 岭回归模型使用 **L2正则化方法** ,即保留所有特征变量，但是减小系数值（改变alpha），让特征变量对预测结果影响变小  
  复杂度越低的模型，在训练数据集上表现越差，其泛化能力就会更好，如果更在意模型在泛化方面，则应选择岭回归模型  
  如果数据足够多，则正则化不是很重要，线性回归和岭回归表现差不多（数据越多，线性回归越不容易过拟合）  
### 套索回归（使用L1正则化）
将系数限制在接近0的范围内，有部分特征系数正好为0（模型自动忽略）  
降低alpha可以拟合出更复杂的模型，降低欠拟合的程度，但是若alpha太小，则相当于把正则化效果去除，可能出现过拟合  

实践中一般选岭回归，如果数据特征过多，但只有一部分真正重要，则套索回归

### 弹性网模型（Elastic Net）
综合套索回归和岭回归的惩罚因子，用户需调整两个参数，L1正则化参数和L2正则化参数
