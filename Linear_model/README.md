## 线性模型

### 一般公式：
y = w[0]x[0] + w[1]x[1] + ... + w[p]x[p] + b      
其中, x[0],x[1]...x[p]为数据集中特征变量的数量；w, b 为模型的参数； y为数据结果预测值
#### 线性模型通过训练数据集确定自身系数和截距
当训练集特征变量大于数据点数量时，线性模型可以对训练数据较好的预测
##
### 线性回归（普通最小二乘法）
当找到训练数据集中y的预测值和其真实值平方差最小的时候返回对应的w和b, 无可调节参数
### 岭回归（改良最小二乘法）
（训练结果可能模型在训练集的得分变小，而测试集的得分比训练集略高）  
  能够避免过拟合， 岭回归模型使用 **L2正则化方法** ,即保留所有特征变量，但是减小系数值（改变alpha），让特征变量对预测结果影响变小  
  复杂度越低的模型，在训练数据集上表现越差，其泛化能力就会更好，如果更在意模型在泛化方面，则应选择岭回归模型  
  如果数据足够多，则正则化不是很重要，线性回归和岭回归表现差不多（数据越多，线性回归越不容易过拟合）  
